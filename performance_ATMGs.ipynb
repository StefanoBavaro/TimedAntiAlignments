{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"v4-khkg8S1aW"},"outputs":[],"source":["import numpy as np\n","import random\n","import pulp as plp\n","import itertools\n","from itertools import product\n","import random\n","from decimal import Decimal, ROUND_HALF_UP\n","\n","def round_to_two_decimal_places(value):\n","    #Rounds a value to two decimal places with ROUND_HALF_UP.\n","    return Decimal(value).quantize(Decimal(\"0.01\"), rounding=ROUND_HALF_UP)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wjFV5xy7S1aY"},"outputs":[],"source":["class pnet:\n","    \"\"\"\n","    Represents a Time Petri Net.\n","\n","    Attributes:\n","    - transitions: List of transitions in the Petri net.\n","    - places: List of places in the Petri net.\n","    - arcs: List of arcs in the Petri net.\n","\n","    To build a petri net, the input has to be in the following format:\n","      atmg = pnet(arcs, intervals)\n","\n","    - where the arcs parameter is a list of couples of strings, representing the set F of the TPN (i.e. couples of place-transition or transition-place)\n","      Places have to start with the letter p, transitions with the letter t followed by a number that indicates the order of the list of intervals, except for the number 0.    \n","      e.g.: arcs = [(\"p0\", \"t1\"), (\"t1\", \"p1\"), (\"t1\", \"p2\"), (\"p1\", \"t2\"), (\"p2\", \"t3\"), (\"t2\",\"p3\"), (\"t3\",\"p4\") ,(\"p3\", \"t4\") , (\"p4\", \"t4\"), (\"t4\", \"p5\")],\n","       \n","    - where intervals is a list of 2d lists that represent the intervals for each transition, in the order defined by the name of the transitions as defined before \n","      e.g. intervals = [[0, 7], [0, 5], [0, 3], [0,1], [0,3], [1,3 ], [1,5] , [0,55], [0,66]] \n","    \n","    Therefore, given pnet(arcs, intervals), the transition t1 is gonna have interval [0,7] (first in the list), t2 has [0,5] and so on.\n","   \"\"\"\n","    transitions = []\n","    places = []\n","    arcs = []\n","    def __init__(self, arcs, intervals):\n","\n","        self.transitions = []\n","        self.places = set()\n","        self.arcs = arcs\n","\n","        transitions = set()\n","        for arc in self.arcs:\n","            if arc[0].startswith('p'):  # If it's a place, add it to the places set\n","                self.places.add(arc[0])\n","            elif arc[0].startswith('t'):  # If it's a transition, add it to the transitions set\n","                transitions.add(arc[0])\n","\n","            if arc[1].startswith('p'):  # If it's a place, add it to the places set\n","                self.places.add(arc[1])\n","            elif arc[1].startswith('t'):  # If it's a transition, add it to the transitions set\n","                transitions.add(arc[1])\n","\n","        # Create a list of transitions ordered by transition number (extracted from the transition name)\n","        ordered_transitions = sorted(transitions, key=lambda t: int(t[1:]))\n","\n","        # Create the transitions + intervals vector\n","        self.transitions = []\n","        for t in ordered_transitions:\n","            # Since transitions are ordered from t1 to t30, we can get the corresponding interval from the intervals vector\n","            index = int(t[1:]) - 1  # The index for intervals is 1-based, so subtract 1 to get 0-based index\n","            self.transitions.append((t, intervals[index]))\n","\n","        self.order_transitions()\n","\n","    def __str__(self):\n","        return f\"Places: {self.places} \\nTransitions: {self.transitions} \\nArcs: {self.arcs}\"\n","\n","    def get_interval(self, idx_transition):\n","        #returns the timestamp interval of a transition\n","        return self.transitions[idx_transition][1]\n","\n","    def get_parents(self, idx_transition):\n","        #returns the direct parents of a transition in the petri net\n","\n","        parents = []\n","        places = []\n","        for arc in self.arcs:\n","            if arc[1] == self.transitions[idx_transition-1][0]:\n","                if arc[0] not in places:\n","                    places.append(arc[0])\n","\n","        for arc in self.arcs:\n","            if arc[1] in places:\n","                if arc[0] not in parents:\n","                    parents.append(arc[0])\n","        return parents\n","\n","    def get_children(self, idx_transition):\n","        #returns the direct successors of a transition in the petri net\n","\n","        children = []\n","        places = []\n","        for arc in self.arcs:\n","            if arc[0] == self.transitions[idx_transition-1][0]:\n","                if arc[1] not in places:\n","                    places.append(arc[1])\n","\n","        for arc in self.arcs:\n","            if arc[0] in places:\n","                if arc[1] not in children:\n","                    children.append(arc[1])\n","        return children\n","\n","    def idx(self, transition):\n","        #returns the transition index\n","        return int(transition[1:])\n","\n","    def dimension(self):\n","        #returns the dimension (#transitions) of the TPN\n","        return len(self.transitions)\n","\n","    def retrieve_mapping(self, temp):\n","        #returns a mapping of the old and new names of the transitions of the petri net\n","\n","        mapping = dict()\n","\n","        partial_transitions = [t[0] for t in self.transitions]\n","        #print(partial_transitions)\n","\n","        for i,transition in enumerate(self.transitions):\n","            after = transition[0]\n","            before = temp[partial_transitions.index(after)]\n","            #print(before, after)\n","            mapping[before] = after\n","        return mapping\n","\n","    def modify_arcs(self, mapping):\n","        #modifies the arcs with the renamed transitions of the sorted petri net\n","\n","        for i, arc in enumerate(self.arcs):\n","            initial = arc[0]\n","            final = arc[1]\n","\n","            #if initial is a string that starts with \"t\"\n","            if initial.startswith(\"t\"):\n","                initial = mapping[initial]\n","\n","            #if final is a string that starts with \"t\"\n","            if final.startswith(\"t\"):\n","                final = mapping[final]\n","\n","            arc = (initial, final)\n","            self.arcs[i] = arc\n","\n","    def order_transitions(self):\n","        #orders and renames the transitions of the generated petri net\n","\n","        ordered_transitions = []\n","        visited = set()\n","\n","        def dfs(transition):\n","            visited.add(transition)\n","            for child in self.get_children(self.idx(transition)):\n","                if child not in visited:\n","                    dfs(child)\n","            ordered_transitions.append(transition)\n","\n","        for transition in self.transitions:\n","            if transition[0] not in visited:\n","                dfs(transition[0])\n","\n","        ordered_transitions.reverse()\n","        mapping = self.retrieve_mapping(ordered_transitions)\n","\n","\n","        #print(mapping)\n","        temp = [(\"t\"+str(i+1), self.get_interval(self.idx(transition)-1)) for i,transition in enumerate(ordered_transitions)]\n","\n","        self.transitions = temp\n","        self.modify_arcs(mapping)\n","\n","        return self.transitions\n","\n","    def find_idx_transition(self, transition):\n","        #returns the index in the transitions list of the specified transition name\n","        \n","        #print(transition)\n","        only_transitions_names = [t[0] for t in self.transitions]\n","        return only_transitions_names.index(transition)\n","\n","    def generate_random_log(self, cardinality):\n","        #Generates a log with n = cardinality random (a random value taken from every interval) traces accepted by the model\n","        log = []\n","\n","        for _ in range(cardinality):\n","            ex = []\n","            for i in range(len(self.transitions)):\n","                if len(self.get_parents(i + 1)) == 0:\n","                    # Generate a random value within the interval and round it to two decimal places\n","                    value = random.uniform(self.get_interval(i)[0], self.get_interval(i)[1])\n","                    rounded_value = round_to_two_decimal_places(value)\n","                    ex.append(float(rounded_value))\n","                else:\n","                    # Get the maximum value from the parent transitions and add to the random value\n","                    list_of_parents_idxs = [self.find_idx_transition(parent) for parent in self.get_parents(i + 1)]\n","                    parent_value = max([ex[idx] for idx in list_of_parents_idxs])\n","\n","                    # Generate the new value and ensure it's rounded to two decimal places\n","                    new_value = random.uniform(self.get_interval(i)[0], self.get_interval(i)[1]) + parent_value\n","                    rounded_value = round_to_two_decimal_places(new_value)\n","                    ex.append(float(rounded_value))\n","            log.append(ex)\n","\n","        return log\n","\n","    def max_point(self):\n","        #returns the maximal point \\sigma_M of the model \n","        max_point = []\n","\n","        for i in range(len(self.transitions)):\n","            if len(self.get_parents(i+1)) == 0:\n","                max_point.append(self.get_interval(i)[1])\n","            else:\n","                list_of_parents_idxs = [self.find_idx_transition(parent) for parent in self.get_parents(i+1)]\n","                parent_value = max([max_point[i] for i in list_of_parents_idxs])\n","                max_point.append( self.get_interval(i)[1] + parent_value)\n","        return max_point\n","\n","    def min_point(self):\n","        #returns the minimal point \\sigma_m of the model \n","\n","        min_point = []\n","\n","        for i in range(len(self.transitions)):\n","            if len(self.get_parents(i+1)) == 0:\n","                min_point.append(self.get_interval(i)[0])\n","            else:\n","                list_of_parents_idxs = [self.find_idx_transition(parent) for parent in self.get_parents(i+1)]\n","                parent_value = max([min_point[i] for i in list_of_parents_idxs])\n","                min_point.append(self.get_interval(i)[0] + parent_value)\n","        return min_point\n","\n","    pass\n","\n","def transform_log(L, model):\n","    #Transforms the log into the equivalent using flow functions (for delay only distance)\n","    for i, trace in enumerate(L):\n","        # Convert each tuple to a list\n","        if len(model.transitions)==1:\n","            trace_list = list([trace])\n","        else:\n","            trace_list = list(trace)\n","\n","        old_trace = trace_list.copy()\n","\n","\n","        for j in range(len(trace_list)):\n","            #print(trace_list[j])\n","            if len(model.get_parents(j+1)) != 0:\n","                list_of_parents_idxs = [model.find_idx_transition(parent) for parent in model.get_parents(j+1)]\n","                parent_value = max([old_trace[i] for i in list_of_parents_idxs])\n","                trace_list[j] = trace_list[j] - parent_value\n","\n","            # Convert the list back to a tuple\n","            L[i] = tuple(trace_list)\n","\n","    return L\n","\n","def search_space_type1(counter, dimension, brought, model, gamma, search_space,step, distance_type):\n","    #build the search space (set of \"all\" points accepted by the model) brute force, for a ATMG as described in the experiments section of the paper\n","    #as the space is continuous, the parameter \"step\" determines the number of splits of the space of each dimension into equidistant points\n","\n","  if counter-1 == dimension:\n","      # Base case: all loops have been executed, do something with the values\n","      search_space.append(gamma.copy())\n","      #print(gamma, search_space)\n","      return search_space\n","  else:\n","      lower = model.get_interval(counter-1)[0] + brought\n","      upper = model.get_interval(counter-1)[1] + brought\n","      for i in  np.linspace(lower, upper, step):\n","          gamma.append(i)  # Modify the values for the current loop\n","\n","          if distance_type == 0:\n","              if(counter in range(2,dimension)):\n","                if counter == dimension-1:\n","                  give = np.max(gamma[1:dimension-1])\n","                else:\n","                  give = gamma[0]\n","                #print(counter, give)\n","              elif(counter == 1):\n","                give = i\n","              elif(counter == dimension):\n","                give = np.max(gamma[1:dimension-1])\n","                #print(gamma, give, counter)\n","              else:\n","                give = np.inf\n","                print(\"error\")\n","              search_space_type1(counter + 1, dimension, give, model , gamma, search_space, step, distance_type)  #Recursively call the function for the next loop\n","          else:\n","              search_space_type1(counter + 1, dimension, 0, model , gamma, search_space, step, distance_type)\n","          gamma.pop()  # Remove the current loop's value\n","\n","  if counter == 1:\n","      print('done')\n","      return search_space\n","\n","def search_space_delay(model, step):\n","  #builds the search space when using the delay only distance\n","  #(discretized, still with the parameter step determining the number of splits of the space of each dimension into equidistant points)\n","\n","  # Create a list of linspaces for each interval\n","  linspaces = [np.linspace(t[1][0], t[1][1], step) for t in model.transitions]\n","\n","  # Generate all combinations of points using itertools.product\n","  combinations = set(product(*linspaces))\n","\n","  return combinations\n","\n","def find_aa(model, L, step, distance_type):\n","    #finds all possible anti-alignments considering the discretized search space, and comparing every point brute force\n","    \n","    dimension = len(model.transitions)\n","    #print('dimension:', dimension)\n","\n","    if distance_type == 0:\n","        search_space = search_space_type1(1, dimension, 0, model, [], [], step, distance_type)\n","    else:\n","        search_space = search_space_delay(model, step)\n","\n","    print(model)\n","    #print('Search space:', search_space)\n","\n","\n","    # Initialize variables\n","    max_distance = float('-inf')\n","    max_points = []\n","\n","    # Convert the list L to a numpy array for efficient vector operations\n","    L_array = np.array(L)\n","\n","    # Iterate over each point in the search space\n","    for gamma in search_space:\n","        # Convert gamma to a numpy array for efficient vector operations\n","        gamma_array = np.array(gamma)\n","\n","        # Calculate the minimum distance for the current gamma\n","        distances = np.linalg.norm(L_array - gamma_array, ord=1, axis=1)  # Efficient L1 norm computation\n","        min_distance = np.min(distances)  # Minimum distance to any point in L\n","\n","        # Update max_distance and max_points based on the minimum distance\n","        if min_distance > max_distance:\n","            max_distance = min_distance\n","            max_points = [gamma]\n","        elif min_distance == max_distance:\n","            max_points.append(gamma)\n","\n","    return max_distance, max_points\n","\n","def brute_force_solver(model, log, step, distance_type):\n","    #the log is to be transformed or not depending on whether the LP solver has already been used for the same log\n","    #in such case, the log is already transformed\n","    #if the BF solver is the only one used, this line has to be uncommented \n","    \"\"\"\n","    if distance_type == 1:\n","        log = transform_log(log, model)\n","    \"\"\"\n","    max_distance, max_points = find_aa(model, log, step, distance_type)\n","    #print('Max Distance:', max_distance)\n","    #print('Optimal Point:', max_points[0])\n","\n","    return max_distance, max_points\n","\n","\n","def gen_pnet_type0(dimension,random_intervals=False):\n","    #returns a petri net with all transitions in parallel and not linked to each other\n","\n","    for i in range(dimension):\n","        if i == 0:\n","            arcs = [(\"p00\", \"t1\"), (\"t1\", \"p01\")]\n","            if random_intervals:\n","                lower_bound =  round(round(random.uniform(0, 10),3), 2)\n","                upper_bound =  lower_bound + round(round(random.uniform(0, 10),3), 2)\n","                intervals= [(lower_bound, upper_bound)]\n","            else:\n","                intervals = [(0, 1)]\n","        else:\n","            arcs.append((\"p\"+str(i)+ \"0\", \"t\"+str(i+1)))\n","            arcs.append((\"t\"+str(i+1), \"p\"+str(i)+ \"1\"))\n","            if random_intervals:\n","                lower_bound =  round(round(random.uniform(0, 10),3), 2)\n","                upper_bound =  lower_bound + round(round(random.uniform(0, 10),3), 2)\n","                intervals.append((lower_bound, upper_bound))\n","            else:\n","                intervals.append((0, 1))\n","\n","\n","    model = pnet(arcs, intervals)\n","    print(model)\n","    return model\n","\n","import json\n","import ast\n","\n","def retrieve_example(cardinality, dimension):\n","    #retrieves the log and model from the JSON file given the cardinality and dimension of the problem\n","    #it retrieves it only for distance type 0 (stamp only), to retrieve the \"original\" log\n","        \n","    filepath = \"/content/drive/MyDrive/BF_PAR_lp_solver_logs.json\"\n","\n","    # Load JSON data from a file\n","    with open(filepath, \"r\") as f:\n","        json_data = json.load(f)\n","\n","    key_pattern = f\"{cardinality}-{dimension}-0\"\n","\n","    # Find the entry with the corresponding key\n","    for key, entry in json_data.items():\n","        if entry.get(\"Key\") == key_pattern:\n","            # Get the Model and Log from the \"Log Data\"\n","            model = ast.literal_eval(entry.get(\"Log Data\", {}).get(\"Model\"))\n","            log = entry.get(\"Log Data\", {}).get(\"Log\")\n","            return model, log\n","\n","    # If the key pattern is not found, return a message\n","    return {\"error\": \"No data found for the given key pattern\"}\n","\n","def LPSolver(model, log, distance_type=0):\n","    #Linear Programming solver with constraints and variables as described in the paper\n","\n","    # Define the dimensionality of the problem\n","    n = model.dimension() #dimension of the space\n","    m = len(log) #number of points in L\n","\n","    if distance_type ==1 : #if delay-only distance is used, the log is transformed in equivalent flow functions traces\n","      log = transform_log(log,model)\n","\n","    #print(model.transitions, log[0])\n","\n","    # Create the LP problem\n","\n","    prob = plp.LpProblem(\"Maximize minimal manhattan distance\", plp.LpMaximize)\n","\n","    # Define the decision variables\n","\n","    x = plp.LpVariable.dicts(\"x\", range(n), lowBound=0, cat = 'Continuous')\n","\n","    #Define the variable to maximize\n","    z = plp.LpVariable(\"z\", lowBound=0, cat = 'Continuous')\n","\n","    #print(x, z)\n","\n","    \n","    M = np.linalg.norm(np.array(model.max_point()) - np.array(model.min_point()), ord=1)\n","    print('M', M)\n","\n","    # Define the constraints for x, i.e. the search space constraints\n","    if distance_type == 0:\n","        max_var = plp.LpVariable.dicts(\"max_var\", range(n), cat = 'Continuous')\n","        for i in range(n):\n","            #print(i)\n","            if len(model.get_parents(i+1)) == 0:\n","                prob += x[i] >= model.get_interval(i)[0]\n","                prob += x[i] <= model.get_interval(i)[1]\n","            else:\n","                binary = plp.LpVariable.dicts(\"binary_\"+str(i), range(len(model.get_parents(i+1))), cat=\"Binary\")\n","                for j, parent in enumerate(model.get_parents(i+1)):\n","                    prob+= max_var[i] >= x[model.idx(parent)-1]\n","                    prob+= max_var[i] <= x[model.idx(parent)-1] + (1-binary[j])*M\n","                    prob += x[i] >= model.get_interval(i)[0] + x[model.idx(parent)-1]\n","                prob += plp.lpSum([binary[j] for j in range(len(model.get_parents(i+1)))]) == 1\n","                prob += x[i] <= model.get_interval(i)[1] + max_var[i]\n","    else:\n","        for i in range(n):\n","            prob += x[i] >= model.get_interval(i)[0]\n","            prob += x[i] <= model.get_interval(i)[1]\n","\n","\n","    # Define the constraints for the absolute value of the difference between x and each point in L\n","    diff_plus = plp.LpVariable.dicts(\"diff_plus\", (range(m), range(n)), lowBound=0, cat = 'Continuous')\n","    diff_minus = plp.LpVariable.dicts(\"diff_minus\", (range(m), range(n)), lowBound=0, cat = 'Continuous')\n","\n","    b = plp.LpVariable.dicts(\"b\", (range(m), range(n)), cat = 'Binary')\n","\n","\n","    for j in range(m): #for every sigma in L\n","        for i in range(n): #for every dimension\n","            if model.dimension() == 1: #if the model is 1-dimensional\n","                prob += diff_plus[j][i] - diff_minus[j][i] == log[j] - x[i]\n","            else:\n","                prob += diff_plus[j][i] - diff_minus[j][i] == log[j][i] - x[i]\n","\n","            prob += diff_plus[j][i] <= M*b[j][i]\n","            prob += diff_minus[j][i] <= M*(1-b[j][i])\n","\n","        prob += z <= plp.lpSum([diff_plus[j][i] + diff_minus[j][i] for i in range(n)])\n","\n","\n","    # Define the objective function\n","    prob += z\n","\n","    #print(prob)\n","\n","    # Solve the problem\n","    prob.solve()\n","\n","    # Print the results\n","    print(\"Status:\", plp.LpStatus[prob.status])\n","    #print(\"Max Distance:\", plp.value(prob.objective))\n","\n","    optimal_point = [plp.value(x[i]) for i in range(n)]\n","    #print('Optimal Point:', optimal_point)\n","\n","    return plp.value(prob.objective), optimal_point, prob.numVariables(), prob.numConstraints()\n","\n","def gen_pnet_type1(dimension, random_intervals):\n","    #generates a random ATMG with the given dimension, with the structure and the extrema of the intervals as explained in the paper (experiments section)\n","\n","    for i in range(dimension):\n","        if i == 0:\n","            arcs = [(\"p00\", \"t1\")]\n","            if random_intervals:\n","                lower_bound =  round(round(random.uniform(0, 5),3), 2)\n","                upper_bound =  lower_bound + round(round(random.uniform(0, 5),3), 2)\n","                intervals= [(lower_bound, upper_bound)]\n","            else:\n","                intervals = [(0, 1)]\n","        elif i == dimension-1:\n","            arcs.append((\"t\"+str(i+1), \"p\"+str(i)+ \"1\"))\n","            if random_intervals:\n","                lower_bound =  round(round(random.uniform(0, 5),3), 2)\n","                upper_bound =  lower_bound + round(round(random.uniform(0, 5),3), 2)\n","                intervals.append((lower_bound, upper_bound))\n","            else:\n","                intervals.append((0, 1))\n","        else:\n","            arcs.append((\"t1\", \"p0\"+str(i)))\n","            arcs.append((\"p0\"+str(i), \"t\"+str(i+1)))\n","            arcs.append((\"t\"+str(i+1), \"p\"+str(i)+ \"1\"))\n","            arcs.append((\"p\"+str(i)+ \"1\", \"t\"+str(dimension)))\n","            if random_intervals:\n","                lower_bound =  round(round(random.uniform(0, 5),3), 2)\n","                upper_bound =  lower_bound + round(round(random.uniform(0, 5),3), 2)\n","                intervals.append((lower_bound, upper_bound))\n","            else:\n","                intervals.append((0, 1))\n","\n","    print(arcs, intervals)\n","    model = pnet(arcs, intervals)\n","    #print(model)\n","    return model\n","\n","def generate_random_example_t1(dimension,cardinality,random_model):\n","        #generates a random example of problem given the parameters of dimension and cardinality, \n","        #i.e. an ATMG with #trns = dimension (and structure as described in the experiment section of the paper) \n","        #and a random log accepted by the model with #traces = cardinality\n","        # if random_model = False, then the model will have all intervals as [0,1]\n","        model = gen_pnet_type1(dimension,random_model)\n","\n","        log = model.generate_random_log(cardinality)\n","        return log, model\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-8nz-JB0FspP","outputId":"cd251b89-8424-478a-8d29-a6228a0b1b0a"},"outputs":[{"name":"stdout","output_type":"stream","text":["[('p00', 't1'), ('t1', 'p01'), ('p01', 't2'), ('t2', 'p11'), ('p11', 't12'), ('t1', 'p02'), ('p02', 't3'), ('t3', 'p21'), ('p21', 't12'), ('t1', 'p03'), ('p03', 't4'), ('t4', 'p31'), ('p31', 't12'), ('t1', 'p04'), ('p04', 't5'), ('t5', 'p41'), ('p41', 't12'), ('t1', 'p05'), ('p05', 't6'), ('t6', 'p51'), ('p51', 't12'), ('t1', 'p06'), ('p06', 't7'), ('t7', 'p61'), ('p61', 't12'), ('t1', 'p07'), ('p07', 't8'), ('t8', 'p71'), ('p71', 't12'), ('t1', 'p08'), ('p08', 't9'), ('t9', 'p81'), ('p81', 't12'), ('t1', 'p09'), ('p09', 't10'), ('t10', 'p91'), ('p91', 't12'), ('t1', 'p010'), ('p010', 't11'), ('t11', 'p101'), ('p101', 't12'), ('t12', 'p111')] [(0.12, 5.08), (3.15, 6.77), (1.14, 4.36), (2.44, 6.970000000000001), (0.01, 3.5199999999999996), (0.37, 4.5200000000000005), (0.97, 4.03), (3.45, 5.07), (0.47, 4.32), (4.91, 7.6), (0.59, 3.5), (2.15, 3.27)]\n","Cardinality: 10, Dimension: 12, Distance Type: 0\n","[('t1', (0.12, 5.08)), ('t2', (0.59, 3.5)), ('t3', (4.91, 7.6)), ('t4', (0.47, 4.32)), ('t5', (3.45, 5.07)), ('t6', (0.97, 4.03)), ('t7', (0.37, 4.5200000000000005)), ('t8', (0.01, 3.5199999999999996)), ('t9', (2.44, 6.970000000000001)), ('t10', (1.14, 4.36)), ('t11', (3.15, 6.77)), ('t12', (2.15, 3.27))] [[1.53, 3.49, 8.8, 2.82, 5.41, 4.0, 5.37, 3.4, 5.22, 4.36, 8.27, 11.99], [2.24, 4.6, 8.91, 5.19, 6.73, 3.65, 6.37, 5.26, 6.64, 4.34, 6.14, 11.56], [4.1, 6.64, 9.48, 8.41, 9.01, 5.72, 4.52, 5.25, 8.87, 7.09, 8.68, 12.52], [3.63, 5.69, 10.62, 6.3, 8.55, 6.0, 5.88, 6.41, 8.29, 5.73, 7.72, 13.39], [4.95, 7.6, 10.56, 8.13, 9.82, 8.16, 9.2, 6.1, 7.74, 8.45, 9.26, 13.41], [3.28, 4.45, 8.39, 5.95, 6.94, 6.58, 6.78, 3.58, 6.9, 5.46, 8.66, 11.33], [0.68, 2.43, 6.16, 2.9, 4.59, 4.4, 4.11, 3.93, 4.41, 4.32, 5.09, 8.87], [2.17, 5.1, 7.83, 4.52, 6.16, 5.37, 2.8, 5.28, 8.03, 3.37, 6.85, 10.98], [1.14, 2.42, 8.49, 2.99, 4.77, 2.82, 5.56, 4.65, 6.46, 2.97, 4.56, 11.5], [3.12, 5.26, 9.28, 6.82, 6.78, 6.03, 4.13, 3.33, 7.92, 4.89, 7.86, 11.77]]\n","96.49\n","Status: Optimal\n"]}],"source":["import time\n","import json\n","import pandas as pd\n","from itertools import product\n","import os\n","\n","# Parameters to generate examples\n","cardinalities = [10]\n","dimensions = [12]\n","distance_types = [0,1]\n","#step = 10\n","\n","# Generate list of pairs of cardinalities and dimensions\n","pairs = product(cardinalities, dimensions)\n","\n","# File paths for storing results\n","json_filename = \"/content/drive/MyDrive/BF_PAR_solver_logs.json\"\n","csv_filename = \"/content/drive/MyDrive/BF_PAR_solver_results.csv\"\n","json_filename2 = \"/content/drive/MyDrive/BF_PAR_solver_logs2.json\"\n","\n","def save_log_to_json(key, log_data, file_path):\n","    #Saves log and model to a JSON file with a unique identifier, along with the key\n","    \n","    # Convert the data to a serializable format\n","    log_data_serializable = {str(k): v for k, v in log_data.items()}\n","\n","    # Generate a unique identifier for the log\n","    log_id = hash(json.dumps(log_data_serializable, sort_keys=True))\n","\n","    # Structure for storing the log with the key\n","    log_entry = {\n","        \"Key\": key,\n","        \"Log Data\": log_data_serializable\n","    }\n","\n","    # Create or update the JSON file\n","    if not os.path.exists(file_path):\n","        with open(file_path, 'w') as f:\n","            json.dump({}, f, indent=4)\n","\n","    # Load existing data\n","    with open(file_path, 'r') as f:\n","        existing_data = json.load(f)\n","\n","    # Add the new log with the unique identifier\n","    existing_data[str(log_id)] = log_entry\n","\n","    # Write back the updated data\n","    with open(file_path, 'w') as f:\n","        json.dump(existing_data, f, indent=4)\n","\n","    return log_id  # Return the unique identifier to reference this log\n","\n","# Function to append to a CSV file\n","def append_results_to_csv(results_dict, file_path):\n","    #Appends data (result) to a CSV file. \n","    df = pd.DataFrame([results_dict])\n","    file_exists = os.path.exists(file_path)\n","\n","    # If the file does not exist or is empty, write headers\n","    if not file_exists or os.path.getsize(file_path) == 0:\n","        df.to_csv(file_path, mode='w', index=False, header=True, sep=';')\n","    else:\n","        df.to_csv(file_path, mode='a', index=False, header=False, sep=';')  # Append without headers\n","\n","\n","# Dictionary to store results\n","lp_results = {}\n","\n","# LP-based experiments loop\n","for cardinality, dimension in pairs:\n","\n","    # Generate\\retrieve example \n","    #model, log, = retrieve_example(cardinality, dimension)\n","    log, model = generate_random_example_t1(dimension, cardinality, random_model = True)\n","\n","    #print(model)\n","\n","\n","    for distance_type in distance_types:\n","        key = f\"{cardinality}-{dimension}-{distance_type}\"\n","        print(f\"Cardinality: {cardinality}, Dimension: {dimension}, Distance Type: {distance_type}\")\n","\n","        # Save the log data to the JSON file and get the unique ID\n","        log_id = save_log_to_json(key, {\"Model\": str(model), \"Log\": log}, json_filename2)\n","        #REMOVE LOG AND MODEL FROM JSON IF SOLUTION NOT ENDED\n","\n","        start_time = time.time()\n","        max_dist_LP, opt_point_LP, numVars, numConstr = LPSolver(model, log, distance_type)\n","        elapsed_time_LP = time.time() - start_time\n","\n","        start_time_BF = time.time()\n","        max_dist_BF, opt_point_BF = brute_force_solver(model, log, 5, distance_type)\n","        elapsed_time_BF = time.time() - start_time_BF\n","\n","        # Save the log data to the JSON file and get the unique ID\n","        log_id = save_log_to_json(key, {\"Model\": str(model), \"Log\": log}, json_filename)\n","        #REMOVE LOG AND MODEL FROM JSON IF SOLUTION NOT ENDED\n","\n","\n","        # Store the results\n","        result_data = {\n","            'Key': key,\n","            'Dimension': dimension,\n","            'Cardinality': cardinality,\n","            'Distance Type': distance_type,\n","            #'Model': str(model),  # Ensure this is serializable\n","            #'Log': str(L),  # Ensure this is serializable\n","            'Elapsed Time (LP)': elapsed_time_LP,\n","            'Elapsed Time (BF)': elapsed_time_BF,\n","            #'Number of Variables (LP)': numVars,\n","            #'Number of Constraints (LP)': numConstr,\n","            'Solution (LP)': str(opt_point_LP),  # Ensure this is serializable\n","            'Solution (BF)': str(opt_point_BF),  # Ensure this is serializable\n","            'Max Distance (LP)': max_dist_LP,\n","            'Max Distance (BF)': max_dist_BF,\n","        }\n","\n","        append_results_to_csv(result_data, csv_filename)\n","\n","\n","        # Display the results\n","        print(f\"Elapsed Time (LP): {elapsed_time_LP:.6f} seconds\")\n","        print(f\"Elapsed Time (BF): {elapsed_time_BF:.6f} seconds\")\n","        #print(f\"Number of Variables (LP): {numVars}\")\n","        #print(f\"Number of Constraints (LP): {numConstr}\")\n","        print(f\"Max Distance (LP): {max_dist_LP}\")\n","        print(f\"Max Distance (BF): {max_dist_BF}\")\n","\n","        print()\n","\n","        # Save the results to both JSON and CSV files\n","        #append_results_to_json(lp_results, json_filename)\n","\n","print(f\"LP-based results successfully appended {csv_filename}.\")"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":0}
